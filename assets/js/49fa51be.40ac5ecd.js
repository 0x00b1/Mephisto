"use strict";(self.webpackChunkweb=self.webpackChunkweb||[]).push([[6731],{5110:function(e,t,r){r.d(t,{Zo:function(){return c},kt:function(){return f}});var o=r(9703);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,o)}return r}function a(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function u(e,t){if(null==e)return{};var r,o,n=function(e,t){if(null==e)return{};var r,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)r=i[o],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)r=i[o],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var l=o.createContext({}),s=function(e){var t=o.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):a(a({},t),e)),r},c=function(e){var t=s(e.components);return o.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var r=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,c=u(e,["components","mdxType","originalType","parentName"]),d=s(r),f=n,w=d["".concat(l,".").concat(f)]||d[f]||p[f]||i;return r?o.createElement(w,a(a({ref:t},c),{},{components:r})):o.createElement(w,a({ref:t},c))}));function f(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=r.length,a=new Array(i);a[0]=d;var u={};for(var l in t)hasOwnProperty.call(t,l)&&(u[l]=t[l]);u.originalType=e,u.mdxType="string"==typeof e?e:n,a[1]=u;for(var s=2;s<i;s++)a[s]=r[s];return o.createElement.apply(null,a)}return o.createElement.apply(null,r)}d.displayName="MDXCreateElement"},1239:function(e,t,r){r.r(t),r.d(t,{frontMatter:function(){return u},contentTitle:function(){return l},metadata:function(){return s},toc:function(){return c},default:function(){return d}});var o=r(3996),n=r(2313),i=(r(9703),r(5110)),a=["components"],u={sidebar_position:4},l="Developing a workflow",s={unversionedId:"guides/tutorials/workflows",id:"guides/tutorials/workflows",isDocsHomePage:!1,title:"Developing a workflow",description:"While it's nice to imagine that you'll be able to collect quality data on the first pass, crowdsourcing can be a bit more trial-and-error. This guide focuses on setting up a good workflow, and extending your run-script to support additional functionality. We'll work through a common workflow for launching a task, checking results, evaluating, and re-running.",source:"@site/docs/guides/tutorials/workflows.md",sourceDirName:"guides/tutorials",slug:"/guides/tutorials/workflows",permalink:"/docs/guides/tutorials/workflows",editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/guides/tutorials/workflows.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"guides",previous:{title:"Introducing worker controls",permalink:"/docs/guides/tutorials/worker_controls"},next:{title:"Using the Mephisto Review CLI",permalink:"/docs/guides/tutorials/review-cli"}},c=[{value:"Proper use of <code>task_name</code>",id:"proper-use-of-task_name",children:[],level:2},{value:"Setting up automated evaluation",id:"setting-up-automated-evaluation",children:[],level:2},{value:"Multi-purpose run scripts",id:"multi-purpose-run-scripts",children:[],level:2},{value:"Using the existing review workflow",id:"using-the-existing-review-workflow",children:[],level:2},{value:"Automated review workflows",id:"automated-review-workflows",children:[],level:2}],p={toc:c};function d(e){var t=e.components,r=(0,n.Z)(e,a);return(0,i.kt)("wrapper",(0,o.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"developing-a-workflow"},"Developing a workflow"),(0,i.kt)("p",null,"While it's nice to imagine that you'll be able to collect quality data on the first pass, crowdsourcing can be a bit more trial-and-error. This guide focuses on setting up a good workflow, and extending your run-script to support additional functionality. We'll work through a common workflow for launching a task, checking results, evaluating, and re-running."),(0,i.kt)("h2",{id:"proper-use-of-task_name"},"Proper use of ",(0,i.kt)("inlineCode",{parentName:"h2"},"task_name")),(0,i.kt)("p",null,"TODO - describe good ways to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"mephisto.blueprint.task_name")," attribute."),(0,i.kt)("h2",{id:"setting-up-automated-evaluation"},"Setting up automated evaluation"),(0,i.kt)("p",null,"TODO - show simple use of registering post-run evaluations"),(0,i.kt)("h2",{id:"multi-purpose-run-scripts"},"Multi-purpose run scripts"),(0,i.kt)("p",null,"TODO - show how to use Hydra arguments to make separate ",(0,i.kt)("inlineCode",{parentName:"p"},"qualifying")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"qualified")," task pools."),(0,i.kt)("h2",{id:"using-the-existing-review-workflow"},"Using the existing review workflow"),(0,i.kt)("p",null,"TODO - show how the existing review workflow could be used"),(0,i.kt)("h2",{id:"automated-review-workflows"},"Automated review workflows"),(0,i.kt)("p",null,"TODO - describe how one would extend the review workflow"))}d.isMDXComponent=!0}}]);